{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                        (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = datasets.CIFAR10(root='../CIFAR10_data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = datasets.CIFAR10(root='../CIFAR10_data', train=False,\n",
    "                                        download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3,64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, lin=0, lout=5):\n",
    "        out = x\n",
    "        if lin < 1 and lout > -1:\n",
    "            out = self.conv1(out)\n",
    "            out = self.bn1(out)\n",
    "            out = F.relu(out)\n",
    "        if lin < 2 and lout > 0:\n",
    "            out = self.layer1(out)\n",
    "        if lin < 3 and lout > 1:\n",
    "            out = self.layer2(out)\n",
    "        if lin < 4 and lout > 2:\n",
    "            out = self.layer3(out)\n",
    "        if lin < 5 and lout > 3:\n",
    "            out = self.layer4(out)\n",
    "        if lout > 4:\n",
    "            out = F.avg_pool2d(out, 4)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(PreActBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet50().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9,\n",
    "                      weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0] #128\n",
    "    # print(x.size())\n",
    "   \n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_y = lam * y + (1 - lam) * y[index]\n",
    "    return mixed_x, mixed_y, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, mixed_y):\n",
    "    return criterion(pred,mixed_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    reg_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs, targets, lam = mixup_data(inputs, targets,\n",
    "                                                       0.2, device)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        targets = torch.round(targets)\n",
    "        # print(targets)\n",
    "        targets = targets.type(torch.cuda.LongTensor)\n",
    "        # targets = torch.LongTensor(targets).to(device)\n",
    "        \n",
    "        loss = mixup_criterion(criterion, outputs, targets)\n",
    "        # print(loss)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum().float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 30 == 0:    \n",
    "            print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return (train_loss/batch_idx, reg_loss/batch_idx, 100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    acc = 100.*correct/total\n",
    "\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    return (test_loss/batch_idx, 100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 2.10109543800354\n",
      "0 391 Loss: 2.101 | Reg: 0.00000 | Acc: 19.531% (25/128)\n",
      "30 2.246603012084961\n",
      "30 391 Loss: 2.126 | Reg: 0.00000 | Acc: 18.548% (736/3968)\n",
      "60 2.1052443981170654\n",
      "60 391 Loss: 2.146 | Reg: 0.00000 | Acc: 17.828% (1392/7808)\n",
      "90 2.108152389526367\n",
      "90 391 Loss: 2.134 | Reg: 0.00000 | Acc: 18.544% (2160/11648)\n",
      "120 2.240495204925537\n",
      "120 391 Loss: 2.138 | Reg: 0.00000 | Acc: 18.601% (2881/15488)\n",
      "150 2.0284171104431152\n",
      "150 391 Loss: 2.139 | Reg: 0.00000 | Acc: 18.600% (3595/19328)\n",
      "180 2.0743987560272217\n",
      "180 391 Loss: 2.133 | Reg: 0.00000 | Acc: 19.156% (4438/23168)\n",
      "210 2.0170626640319824\n",
      "210 391 Loss: 2.131 | Reg: 0.00000 | Acc: 19.387% (5236/27008)\n",
      "240 2.1502742767333984\n",
      "240 391 Loss: 2.127 | Reg: 0.00000 | Acc: 19.577% (6039/30848)\n",
      "270 2.0610275268554688\n",
      "270 391 Loss: 2.120 | Reg: 0.00000 | Acc: 19.984% (6932/34688)\n",
      "300 2.013003349304199\n",
      "300 391 Loss: 2.116 | Reg: 0.00000 | Acc: 20.097% (7743/38528)\n",
      "330 2.1980440616607666\n",
      "330 391 Loss: 2.113 | Reg: 0.00000 | Acc: 20.383% (8636/42368)\n",
      "360 2.05249285697937\n",
      "360 391 Loss: 2.105 | Reg: 0.00000 | Acc: 20.719% (9574/46208)\n",
      "390 2.0355682373046875\n",
      "390 391 Loss: 2.100 | Reg: 0.00000 | Acc: 21.000% (10500/50000)\n",
      "0 500 Loss: 2.088 | Acc: 23.000% (23/100)\n",
      "10 500 Loss: 1.983 | Acc: 27.636% (304/1100)\n",
      "20 500 Loss: 1.993 | Acc: 27.286% (573/2100)\n",
      "30 500 Loss: 1.984 | Acc: 27.355% (848/3100)\n",
      "40 500 Loss: 1.981 | Acc: 27.000% (1107/4100)\n",
      "50 500 Loss: 1.979 | Acc: 27.412% (1398/5100)\n",
      "60 500 Loss: 1.973 | Acc: 27.377% (1670/6100)\n",
      "70 500 Loss: 1.972 | Acc: 27.366% (1943/7100)\n",
      "80 500 Loss: 1.976 | Acc: 27.099% (2195/8100)\n",
      "90 500 Loss: 1.978 | Acc: 26.923% (2450/9100)\n",
      "100 500 Loss: 1.974 | Acc: 26.822% (2709/10100)\n",
      "110 500 Loss: 1.974 | Acc: 27.090% (3007/11100)\n",
      "120 500 Loss: 1.974 | Acc: 27.215% (3293/12100)\n",
      "130 500 Loss: 1.972 | Acc: 27.214% (3565/13100)\n",
      "140 500 Loss: 1.972 | Acc: 27.241% (3841/14100)\n",
      "150 500 Loss: 1.973 | Acc: 27.192% (4106/15100)\n",
      "160 500 Loss: 1.977 | Acc: 27.031% (4352/16100)\n",
      "170 500 Loss: 1.977 | Acc: 27.058% (4627/17100)\n",
      "180 500 Loss: 1.977 | Acc: 27.072% (4900/18100)\n",
      "190 500 Loss: 1.977 | Acc: 27.194% (5194/19100)\n",
      "200 500 Loss: 1.978 | Acc: 27.169% (5461/20100)\n",
      "210 500 Loss: 1.977 | Acc: 27.171% (5733/21100)\n",
      "220 500 Loss: 1.977 | Acc: 27.213% (6014/22100)\n",
      "230 500 Loss: 1.977 | Acc: 27.208% (6285/23100)\n",
      "240 500 Loss: 1.980 | Acc: 27.303% (6580/24100)\n",
      "250 500 Loss: 1.980 | Acc: 27.291% (6850/25100)\n",
      "260 500 Loss: 1.979 | Acc: 27.314% (7129/26100)\n",
      "270 500 Loss: 1.980 | Acc: 27.295% (7397/27100)\n",
      "280 500 Loss: 1.982 | Acc: 27.285% (7667/28100)\n",
      "290 500 Loss: 1.982 | Acc: 27.227% (7923/29100)\n",
      "300 500 Loss: 1.982 | Acc: 27.209% (8190/30100)\n",
      "310 500 Loss: 1.982 | Acc: 27.100% (8428/31100)\n",
      "320 500 Loss: 1.982 | Acc: 27.143% (8713/32100)\n",
      "330 500 Loss: 1.981 | Acc: 27.115% (8975/33100)\n",
      "340 500 Loss: 1.981 | Acc: 27.067% (9230/34100)\n",
      "350 500 Loss: 1.981 | Acc: 27.083% (9506/35100)\n",
      "360 500 Loss: 1.980 | Acc: 27.119% (9790/36100)\n",
      "370 500 Loss: 1.979 | Acc: 27.127% (10064/37100)\n",
      "380 500 Loss: 1.980 | Acc: 27.105% (10327/38100)\n",
      "390 500 Loss: 1.979 | Acc: 27.136% (10610/39100)\n",
      "400 500 Loss: 1.979 | Acc: 27.120% (10875/40100)\n",
      "410 500 Loss: 1.979 | Acc: 27.112% (11143/41100)\n",
      "420 500 Loss: 1.979 | Acc: 27.067% (11395/42100)\n",
      "430 500 Loss: 1.979 | Acc: 27.063% (11664/43100)\n",
      "440 500 Loss: 1.978 | Acc: 27.048% (11928/44100)\n",
      "450 500 Loss: 1.978 | Acc: 27.011% (12182/45100)\n",
      "460 500 Loss: 1.979 | Acc: 26.976% (12436/46100)\n",
      "470 500 Loss: 1.979 | Acc: 26.930% (12684/47100)\n",
      "480 500 Loss: 1.978 | Acc: 26.933% (12955/48100)\n",
      "490 500 Loss: 1.978 | Acc: 26.978% (13246/49100)\n",
      "\n",
      "Epoch: 1\n",
      "0 2.0580050945281982\n",
      "0 391 Loss: 2.058 | Reg: 0.00000 | Acc: 25.781% (33/128)\n",
      "30 2.0553150177001953\n",
      "30 391 Loss: 2.077 | Reg: 0.00000 | Acc: 23.715% (941/3968)\n",
      "60 2.518038034439087\n",
      "60 391 Loss: 2.060 | Reg: 0.00000 | Acc: 23.899% (1866/7808)\n",
      "90 1.9049630165100098\n",
      "90 391 Loss: 2.045 | Reg: 0.00000 | Acc: 24.184% (2817/11648)\n",
      "120 2.0115132331848145\n",
      "120 391 Loss: 2.042 | Reg: 0.00000 | Acc: 24.206% (3749/15488)\n",
      "150 1.8648905754089355\n",
      "150 391 Loss: 2.024 | Reg: 0.00000 | Acc: 24.943% (4821/19328)\n",
      "180 2.1164302825927734\n",
      "180 391 Loss: 2.038 | Reg: 0.00000 | Acc: 24.240% (5616/23168)\n",
      "210 1.8523684740066528\n",
      "210 391 Loss: 2.037 | Reg: 0.00000 | Acc: 24.245% (6548/27008)\n",
      "240 1.925236701965332\n",
      "240 391 Loss: 2.032 | Reg: 0.00000 | Acc: 24.355% (7513/30848)\n",
      "270 1.796170949935913\n",
      "270 391 Loss: 2.026 | Reg: 0.00000 | Acc: 24.599% (8533/34688)\n",
      "300 1.7841269969940186\n",
      "300 391 Loss: 2.021 | Reg: 0.00000 | Acc: 24.727% (9527/38528)\n",
      "330 2.054743766784668\n",
      "330 391 Loss: 2.017 | Reg: 0.00000 | Acc: 24.927% (10561/42368)\n",
      "360 1.7785584926605225\n",
      "360 391 Loss: 2.016 | Reg: 0.00000 | Acc: 24.959% (11533/46208)\n",
      "390 1.9716764688491821\n",
      "390 391 Loss: 2.010 | Reg: 0.00000 | Acc: 25.120% (12560/50000)\n",
      "0 500 Loss: 1.839 | Acc: 35.000% (35/100)\n",
      "10 500 Loss: 1.790 | Acc: 31.545% (347/1100)\n",
      "20 500 Loss: 1.814 | Acc: 32.000% (672/2100)\n",
      "30 500 Loss: 1.816 | Acc: 31.806% (986/3100)\n",
      "40 500 Loss: 1.819 | Acc: 31.512% (1292/4100)\n",
      "50 500 Loss: 1.817 | Acc: 31.686% (1616/5100)\n",
      "60 500 Loss: 1.811 | Acc: 31.918% (1947/6100)\n",
      "70 500 Loss: 1.808 | Acc: 31.831% (2260/7100)\n",
      "80 500 Loss: 1.811 | Acc: 31.728% (2570/8100)\n",
      "90 500 Loss: 1.813 | Acc: 31.703% (2885/9100)\n",
      "100 500 Loss: 1.811 | Acc: 31.614% (3193/10100)\n",
      "110 500 Loss: 1.812 | Acc: 31.667% (3515/11100)\n",
      "120 500 Loss: 1.812 | Acc: 31.711% (3837/12100)\n",
      "130 500 Loss: 1.811 | Acc: 31.756% (4160/13100)\n",
      "140 500 Loss: 1.811 | Acc: 31.574% (4452/14100)\n",
      "150 500 Loss: 1.812 | Acc: 31.570% (4767/15100)\n",
      "160 500 Loss: 1.816 | Acc: 31.522% (5075/16100)\n",
      "170 500 Loss: 1.815 | Acc: 31.433% (5375/17100)\n",
      "180 500 Loss: 1.816 | Acc: 31.403% (5684/18100)\n",
      "190 500 Loss: 1.817 | Acc: 31.403% (5998/19100)\n",
      "200 500 Loss: 1.816 | Acc: 31.522% (6336/20100)\n",
      "210 500 Loss: 1.816 | Acc: 31.493% (6645/21100)\n",
      "220 500 Loss: 1.814 | Acc: 31.643% (6993/22100)\n",
      "230 500 Loss: 1.814 | Acc: 31.688% (7320/23100)\n",
      "240 500 Loss: 1.813 | Acc: 31.734% (7648/24100)\n",
      "250 500 Loss: 1.813 | Acc: 31.785% (7978/25100)\n",
      "260 500 Loss: 1.812 | Acc: 31.889% (8323/26100)\n",
      "270 500 Loss: 1.813 | Acc: 31.904% (8646/27100)\n",
      "280 500 Loss: 1.814 | Acc: 31.911% (8967/28100)\n",
      "290 500 Loss: 1.812 | Acc: 31.959% (9300/29100)\n",
      "300 500 Loss: 1.813 | Acc: 32.030% (9641/30100)\n",
      "310 500 Loss: 1.813 | Acc: 31.984% (9947/31100)\n",
      "320 500 Loss: 1.814 | Acc: 31.978% (10265/32100)\n",
      "330 500 Loss: 1.814 | Acc: 31.876% (10551/33100)\n",
      "340 500 Loss: 1.815 | Acc: 31.883% (10872/34100)\n",
      "350 500 Loss: 1.814 | Acc: 31.897% (11196/35100)\n",
      "360 500 Loss: 1.814 | Acc: 31.953% (11535/36100)\n",
      "370 500 Loss: 1.814 | Acc: 31.962% (11858/37100)\n",
      "380 500 Loss: 1.815 | Acc: 31.932% (12166/38100)\n",
      "390 500 Loss: 1.815 | Acc: 31.946% (12491/39100)\n",
      "400 500 Loss: 1.815 | Acc: 31.928% (12803/40100)\n",
      "410 500 Loss: 1.816 | Acc: 31.866% (13097/41100)\n",
      "420 500 Loss: 1.816 | Acc: 31.879% (13421/42100)\n",
      "430 500 Loss: 1.815 | Acc: 31.870% (13736/43100)\n",
      "440 500 Loss: 1.815 | Acc: 31.889% (14063/44100)\n",
      "450 500 Loss: 1.815 | Acc: 31.922% (14397/45100)\n",
      "460 500 Loss: 1.815 | Acc: 31.861% (14688/46100)\n",
      "470 500 Loss: 1.815 | Acc: 31.839% (14996/47100)\n",
      "480 500 Loss: 1.815 | Acc: 31.865% (15327/48100)\n",
      "490 500 Loss: 1.815 | Acc: 31.837% (15632/49100)\n",
      "\n",
      "Epoch: 2\n",
      "0 2.2883522510528564\n",
      "0 391 Loss: 2.288 | Reg: 0.00000 | Acc: 17.188% (22/128)\n",
      "30 2.2123570442199707\n",
      "30 391 Loss: 2.037 | Reg: 0.00000 | Acc: 23.614% (937/3968)\n",
      "60 1.8459100723266602\n",
      "60 391 Loss: 2.010 | Reg: 0.00000 | Acc: 24.436% (1908/7808)\n",
      "90 1.9467930793762207\n",
      "90 391 Loss: 1.987 | Reg: 0.00000 | Acc: 25.558% (2977/11648)\n",
      "120 1.783570647239685\n",
      "120 391 Loss: 1.970 | Reg: 0.00000 | Acc: 26.259% (4067/15488)\n",
      "150 2.463707685470581\n",
      "150 391 Loss: 1.953 | Reg: 0.00000 | Acc: 26.940% (5207/19328)\n",
      "180 2.0937016010284424\n",
      "180 391 Loss: 1.956 | Reg: 0.00000 | Acc: 26.653% (6175/23168)\n",
      "210 1.6937328577041626\n",
      "210 391 Loss: 1.945 | Reg: 0.00000 | Acc: 27.048% (7305/27008)\n",
      "240 1.9112094640731812\n",
      "240 391 Loss: 1.937 | Reg: 0.00000 | Acc: 27.593% (8512/30848)\n",
      "270 2.1353695392608643\n",
      "270 391 Loss: 1.938 | Reg: 0.00000 | Acc: 27.638% (9587/34688)\n",
      "300 1.8910727500915527\n",
      "300 391 Loss: 1.935 | Reg: 0.00000 | Acc: 27.876% (10740/38528)\n",
      "330 2.186807870864868\n",
      "330 391 Loss: 1.928 | Reg: 0.00000 | Acc: 28.172% (11936/42368)\n",
      "360 2.1217947006225586\n",
      "360 391 Loss: 1.932 | Reg: 0.00000 | Acc: 28.103% (12986/46208)\n",
      "390 1.6395370960235596\n",
      "390 391 Loss: 1.923 | Reg: 0.00000 | Acc: 28.400% (14200/50000)\n",
      "0 500 Loss: 1.784 | Acc: 38.000% (38/100)\n",
      "10 500 Loss: 1.680 | Acc: 37.455% (412/1100)\n",
      "20 500 Loss: 1.701 | Acc: 37.952% (797/2100)\n",
      "30 500 Loss: 1.711 | Acc: 37.484% (1162/3100)\n",
      "40 500 Loss: 1.725 | Acc: 36.805% (1509/4100)\n",
      "50 500 Loss: 1.715 | Acc: 37.353% (1905/5100)\n",
      "60 500 Loss: 1.711 | Acc: 37.164% (2267/6100)\n",
      "70 500 Loss: 1.707 | Acc: 37.352% (2652/7100)\n",
      "80 500 Loss: 1.706 | Acc: 37.568% (3043/8100)\n",
      "90 500 Loss: 1.707 | Acc: 37.582% (3420/9100)\n",
      "100 500 Loss: 1.704 | Acc: 37.525% (3790/10100)\n",
      "110 500 Loss: 1.705 | Acc: 37.586% (4172/11100)\n",
      "120 500 Loss: 1.704 | Acc: 37.628% (4553/12100)\n",
      "130 500 Loss: 1.702 | Acc: 37.687% (4937/13100)\n",
      "140 500 Loss: 1.703 | Acc: 37.574% (5298/14100)\n",
      "150 500 Loss: 1.708 | Acc: 37.563% (5672/15100)\n",
      "160 500 Loss: 1.713 | Acc: 37.509% (6039/16100)\n",
      "170 500 Loss: 1.711 | Acc: 37.526% (6417/17100)\n",
      "180 500 Loss: 1.711 | Acc: 37.481% (6784/18100)\n",
      "190 500 Loss: 1.712 | Acc: 37.419% (7147/19100)\n",
      "200 500 Loss: 1.714 | Acc: 37.373% (7512/20100)\n",
      "210 500 Loss: 1.714 | Acc: 37.389% (7889/21100)\n",
      "220 500 Loss: 1.713 | Acc: 37.462% (8279/22100)\n",
      "230 500 Loss: 1.713 | Acc: 37.489% (8660/23100)\n",
      "240 500 Loss: 1.712 | Acc: 37.585% (9058/24100)\n",
      "250 500 Loss: 1.711 | Acc: 37.554% (9426/25100)\n",
      "260 500 Loss: 1.711 | Acc: 37.559% (9803/26100)\n",
      "270 500 Loss: 1.712 | Acc: 37.498% (10162/27100)\n",
      "280 500 Loss: 1.713 | Acc: 37.445% (10522/28100)\n",
      "290 500 Loss: 1.712 | Acc: 37.464% (10902/29100)\n",
      "300 500 Loss: 1.712 | Acc: 37.445% (11271/30100)\n",
      "310 500 Loss: 1.713 | Acc: 37.396% (11630/31100)\n",
      "320 500 Loss: 1.714 | Acc: 37.380% (11999/32100)\n",
      "330 500 Loss: 1.715 | Acc: 37.245% (12328/33100)\n",
      "340 500 Loss: 1.716 | Acc: 37.282% (12713/34100)\n",
      "350 500 Loss: 1.715 | Acc: 37.299% (13092/35100)\n",
      "360 500 Loss: 1.715 | Acc: 37.305% (13467/36100)\n",
      "370 500 Loss: 1.715 | Acc: 37.332% (13850/37100)\n",
      "380 500 Loss: 1.716 | Acc: 37.307% (14214/38100)\n",
      "390 500 Loss: 1.716 | Acc: 37.304% (14586/39100)\n",
      "400 500 Loss: 1.716 | Acc: 37.282% (14950/40100)\n",
      "410 500 Loss: 1.717 | Acc: 37.246% (15308/41100)\n",
      "420 500 Loss: 1.717 | Acc: 37.254% (15684/42100)\n",
      "430 500 Loss: 1.717 | Acc: 37.211% (16038/43100)\n",
      "440 500 Loss: 1.717 | Acc: 37.204% (16407/44100)\n",
      "450 500 Loss: 1.716 | Acc: 37.202% (16778/45100)\n",
      "460 500 Loss: 1.716 | Acc: 37.195% (17147/46100)\n",
      "470 500 Loss: 1.716 | Acc: 37.172% (17508/47100)\n",
      "480 500 Loss: 1.716 | Acc: 37.156% (17872/48100)\n",
      "490 500 Loss: 1.716 | Acc: 37.134% (18233/49100)\n",
      "\n",
      "Epoch: 3\n",
      "0 1.6916441917419434\n",
      "0 391 Loss: 1.692 | Reg: 0.00000 | Acc: 35.156% (45/128)\n",
      "30 1.9389126300811768\n",
      "30 391 Loss: 1.876 | Reg: 0.00000 | Acc: 31.376% (1245/3968)\n",
      "60 2.002016067504883\n",
      "60 391 Loss: 1.908 | Reg: 0.00000 | Acc: 29.803% (2327/7808)\n",
      "90 1.5328454971313477\n",
      "90 391 Loss: 1.907 | Reg: 0.00000 | Acc: 29.636% (3452/11648)\n",
      "120 2.10247540473938\n",
      "120 391 Loss: 1.918 | Reg: 0.00000 | Acc: 29.455% (4562/15488)\n",
      "150 1.7576380968093872\n",
      "150 391 Loss: 1.907 | Reg: 0.00000 | Acc: 29.889% (5777/19328)\n",
      "180 1.5524566173553467\n",
      "180 391 Loss: 1.904 | Reg: 0.00000 | Acc: 29.959% (6941/23168)\n",
      "210 1.7994507551193237\n",
      "210 391 Loss: 1.903 | Reg: 0.00000 | Acc: 29.913% (8079/27008)\n",
      "240 1.7678066492080688\n",
      "240 391 Loss: 1.897 | Reg: 0.00000 | Acc: 30.219% (9322/30848)\n",
      "270 1.9532239437103271\n",
      "270 391 Loss: 1.890 | Reg: 0.00000 | Acc: 30.446% (10561/34688)\n",
      "300 2.1352040767669678\n",
      "300 391 Loss: 1.881 | Reg: 0.00000 | Acc: 30.728% (11839/38528)\n",
      "330 2.2886297702789307\n",
      "330 391 Loss: 1.871 | Reg: 0.00000 | Acc: 31.127% (13188/42368)\n",
      "360 1.5200029611587524\n",
      "360 391 Loss: 1.864 | Reg: 0.00000 | Acc: 31.354% (14488/46208)\n",
      "390 1.5841203927993774\n",
      "390 391 Loss: 1.862 | Reg: 0.00000 | Acc: 31.506% (15753/50000)\n",
      "0 500 Loss: 1.720 | Acc: 38.000% (38/100)\n",
      "10 500 Loss: 1.642 | Acc: 40.909% (450/1100)\n",
      "20 500 Loss: 1.670 | Acc: 40.619% (853/2100)\n",
      "30 500 Loss: 1.670 | Acc: 40.484% (1255/3100)\n",
      "40 500 Loss: 1.677 | Acc: 39.707% (1628/4100)\n",
      "50 500 Loss: 1.675 | Acc: 39.725% (2026/5100)\n",
      "60 500 Loss: 1.672 | Acc: 39.803% (2428/6100)\n",
      "70 500 Loss: 1.669 | Acc: 39.704% (2819/7100)\n",
      "80 500 Loss: 1.667 | Acc: 39.679% (3214/8100)\n",
      "90 500 Loss: 1.668 | Acc: 39.659% (3609/9100)\n",
      "100 500 Loss: 1.667 | Acc: 39.851% (4025/10100)\n",
      "110 500 Loss: 1.670 | Acc: 39.847% (4423/11100)\n",
      "120 500 Loss: 1.668 | Acc: 39.893% (4827/12100)\n",
      "130 500 Loss: 1.667 | Acc: 39.885% (5225/13100)\n",
      "140 500 Loss: 1.670 | Acc: 39.766% (5607/14100)\n",
      "150 500 Loss: 1.673 | Acc: 39.755% (6003/15100)\n",
      "160 500 Loss: 1.677 | Acc: 39.634% (6381/16100)\n",
      "170 500 Loss: 1.675 | Acc: 39.637% (6778/17100)\n",
      "180 500 Loss: 1.676 | Acc: 39.569% (7162/18100)\n",
      "190 500 Loss: 1.677 | Acc: 39.503% (7545/19100)\n",
      "200 500 Loss: 1.677 | Acc: 39.483% (7936/20100)\n",
      "210 500 Loss: 1.677 | Acc: 39.502% (8335/21100)\n",
      "220 500 Loss: 1.676 | Acc: 39.620% (8756/22100)\n",
      "230 500 Loss: 1.676 | Acc: 39.589% (9145/23100)\n",
      "240 500 Loss: 1.675 | Acc: 39.747% (9579/24100)\n",
      "250 500 Loss: 1.673 | Acc: 39.813% (9993/25100)\n",
      "260 500 Loss: 1.673 | Acc: 39.854% (10402/26100)\n",
      "270 500 Loss: 1.673 | Acc: 39.867% (10804/27100)\n",
      "280 500 Loss: 1.673 | Acc: 39.751% (11170/28100)\n",
      "290 500 Loss: 1.672 | Acc: 39.794% (11580/29100)\n",
      "300 500 Loss: 1.672 | Acc: 39.814% (11984/30100)\n",
      "310 500 Loss: 1.673 | Acc: 39.736% (12358/31100)\n",
      "320 500 Loss: 1.673 | Acc: 39.720% (12750/32100)\n",
      "330 500 Loss: 1.675 | Acc: 39.634% (13119/33100)\n",
      "340 500 Loss: 1.677 | Acc: 39.566% (13492/34100)\n",
      "350 500 Loss: 1.675 | Acc: 39.627% (13909/35100)\n",
      "360 500 Loss: 1.676 | Acc: 39.609% (14299/36100)\n",
      "370 500 Loss: 1.676 | Acc: 39.666% (14716/37100)\n",
      "380 500 Loss: 1.677 | Acc: 39.661% (15111/38100)\n",
      "390 500 Loss: 1.676 | Acc: 39.683% (15516/39100)\n",
      "400 500 Loss: 1.676 | Acc: 39.668% (15907/40100)\n",
      "410 500 Loss: 1.677 | Acc: 39.630% (16288/41100)\n",
      "420 500 Loss: 1.677 | Acc: 39.627% (16683/42100)\n",
      "430 500 Loss: 1.676 | Acc: 39.610% (17072/43100)\n",
      "440 500 Loss: 1.676 | Acc: 39.560% (17446/44100)\n",
      "450 500 Loss: 1.677 | Acc: 39.570% (17846/45100)\n",
      "460 500 Loss: 1.676 | Acc: 39.579% (18246/46100)\n",
      "470 500 Loss: 1.677 | Acc: 39.527% (18617/47100)\n",
      "480 500 Loss: 1.677 | Acc: 39.507% (19003/48100)\n",
      "490 500 Loss: 1.676 | Acc: 39.532% (19410/49100)\n",
      "\n",
      "Epoch: 4\n",
      "0 1.6733590364456177\n",
      "0 391 Loss: 1.673 | Reg: 0.00000 | Acc: 39.844% (51/128)\n",
      "30 1.9518293142318726\n",
      "30 391 Loss: 1.777 | Reg: 0.00000 | Acc: 35.433% (1406/3968)\n",
      "60 2.1344635486602783\n",
      "60 391 Loss: 1.788 | Reg: 0.00000 | Acc: 34.798% (2717/7808)\n",
      "90 1.6990585327148438\n",
      "90 391 Loss: 1.804 | Reg: 0.00000 | Acc: 33.877% (3946/11648)\n",
      "120 1.6886793375015259\n",
      "120 391 Loss: 1.796 | Reg: 0.00000 | Acc: 34.388% (5326/15488)\n",
      "150 1.690509557723999\n",
      "150 391 Loss: 1.791 | Reg: 0.00000 | Acc: 34.510% (6670/19328)\n",
      "180 1.6438664197921753\n",
      "180 391 Loss: 1.783 | Reg: 0.00000 | Acc: 34.828% (8069/23168)\n",
      "210 1.6532243490219116\n",
      "210 391 Loss: 1.787 | Reg: 0.00000 | Acc: 34.708% (9374/27008)\n",
      "240 1.3614468574523926\n",
      "240 391 Loss: 1.781 | Reg: 0.00000 | Acc: 35.046% (10811/30848)\n",
      "270 1.5504881143569946\n",
      "270 391 Loss: 1.784 | Reg: 0.00000 | Acc: 35.168% (12199/34688)\n",
      "300 1.5249027013778687\n",
      "300 391 Loss: 1.780 | Reg: 0.00000 | Acc: 35.338% (13615/38528)\n",
      "330 1.5983855724334717\n",
      "330 391 Loss: 1.774 | Reg: 0.00000 | Acc: 35.628% (15095/42368)\n",
      "360 1.8203784227371216\n",
      "360 391 Loss: 1.766 | Reg: 0.00000 | Acc: 35.974% (16623/46208)\n",
      "390 1.562689185142517\n",
      "390 391 Loss: 1.766 | Reg: 0.00000 | Acc: 36.064% (18032/50000)\n",
      "0 500 Loss: 1.568 | Acc: 50.000% (50/100)\n",
      "10 500 Loss: 1.549 | Acc: 47.909% (527/1100)\n",
      "20 500 Loss: 1.555 | Acc: 47.048% (988/2100)\n",
      "30 500 Loss: 1.549 | Acc: 46.871% (1453/3100)\n",
      "40 500 Loss: 1.544 | Acc: 46.659% (1913/4100)\n",
      "50 500 Loss: 1.548 | Acc: 46.529% (2373/5100)\n",
      "60 500 Loss: 1.540 | Acc: 46.623% (2844/6100)\n",
      "70 500 Loss: 1.544 | Acc: 46.423% (3296/7100)\n",
      "80 500 Loss: 1.546 | Acc: 46.210% (3743/8100)\n",
      "90 500 Loss: 1.543 | Acc: 46.374% (4220/9100)\n",
      "100 500 Loss: 1.542 | Acc: 46.129% (4659/10100)\n",
      "110 500 Loss: 1.545 | Acc: 45.901% (5095/11100)\n",
      "120 500 Loss: 1.546 | Acc: 45.983% (5564/12100)\n",
      "130 500 Loss: 1.547 | Acc: 45.863% (6008/13100)\n",
      "140 500 Loss: 1.551 | Acc: 45.887% (6470/14100)\n",
      "150 500 Loss: 1.552 | Acc: 45.762% (6910/15100)\n",
      "160 500 Loss: 1.558 | Acc: 45.553% (7334/16100)\n",
      "170 500 Loss: 1.556 | Acc: 45.702% (7815/17100)\n",
      "180 500 Loss: 1.556 | Acc: 45.757% (8282/18100)\n",
      "190 500 Loss: 1.555 | Acc: 45.890% (8765/19100)\n",
      "200 500 Loss: 1.556 | Acc: 45.891% (9224/20100)\n",
      "210 500 Loss: 1.556 | Acc: 45.829% (9670/21100)\n",
      "220 500 Loss: 1.554 | Acc: 45.896% (10143/22100)\n",
      "230 500 Loss: 1.554 | Acc: 45.866% (10595/23100)\n",
      "240 500 Loss: 1.553 | Acc: 45.880% (11057/24100)\n",
      "250 500 Loss: 1.553 | Acc: 45.904% (11522/25100)\n",
      "260 500 Loss: 1.552 | Acc: 45.958% (11995/26100)\n",
      "270 500 Loss: 1.552 | Acc: 45.893% (12437/27100)\n",
      "280 500 Loss: 1.553 | Acc: 45.840% (12881/28100)\n",
      "290 500 Loss: 1.551 | Acc: 45.986% (13382/29100)\n",
      "300 500 Loss: 1.551 | Acc: 45.963% (13835/30100)\n",
      "310 500 Loss: 1.550 | Acc: 45.968% (14296/31100)\n",
      "320 500 Loss: 1.550 | Acc: 45.978% (14759/32100)\n",
      "330 500 Loss: 1.552 | Acc: 45.921% (15200/33100)\n",
      "340 500 Loss: 1.553 | Acc: 45.877% (15644/34100)\n",
      "350 500 Loss: 1.552 | Acc: 45.803% (16077/35100)\n",
      "360 500 Loss: 1.552 | Acc: 45.745% (16514/36100)\n",
      "370 500 Loss: 1.553 | Acc: 45.736% (16968/37100)\n",
      "380 500 Loss: 1.554 | Acc: 45.672% (17401/38100)\n",
      "390 500 Loss: 1.553 | Acc: 45.675% (17859/39100)\n",
      "400 500 Loss: 1.553 | Acc: 45.671% (18314/40100)\n",
      "410 500 Loss: 1.555 | Acc: 45.674% (18772/41100)\n",
      "420 500 Loss: 1.554 | Acc: 45.701% (19240/42100)\n",
      "430 500 Loss: 1.554 | Acc: 45.729% (19709/43100)\n",
      "440 500 Loss: 1.553 | Acc: 45.744% (20173/44100)\n",
      "450 500 Loss: 1.554 | Acc: 45.747% (20632/45100)\n",
      "460 500 Loss: 1.553 | Acc: 45.714% (21074/46100)\n",
      "470 500 Loss: 1.554 | Acc: 45.696% (21523/47100)\n",
      "480 500 Loss: 1.554 | Acc: 45.653% (21959/48100)\n",
      "490 500 Loss: 1.553 | Acc: 45.670% (22424/49100)\n",
      "\n",
      "Epoch: 5\n",
      "0 1.5791784524917603\n",
      "0 391 Loss: 1.579 | Reg: 0.00000 | Acc: 42.188% (54/128)\n",
      "30 2.01641583442688\n",
      "30 391 Loss: 1.717 | Reg: 0.00000 | Acc: 38.911% (1544/3968)\n",
      "60 1.6192271709442139\n",
      "60 391 Loss: 1.703 | Reg: 0.00000 | Acc: 39.575% (3090/7808)\n",
      "90 1.540393352508545\n",
      "90 391 Loss: 1.716 | Reg: 0.00000 | Acc: 38.942% (4536/11648)\n",
      "120 1.4026155471801758\n",
      "120 391 Loss: 1.708 | Reg: 0.00000 | Acc: 39.069% (6051/15488)\n",
      "150 1.8720176219940186\n",
      "150 391 Loss: 1.700 | Reg: 0.00000 | Acc: 39.347% (7605/19328)\n",
      "180 1.4033159017562866\n",
      "180 391 Loss: 1.699 | Reg: 0.00000 | Acc: 39.222% (9087/23168)\n",
      "210 1.5143167972564697\n",
      "210 391 Loss: 1.710 | Reg: 0.00000 | Acc: 38.740% (10463/27008)\n",
      "240 1.4702681303024292\n",
      "240 391 Loss: 1.694 | Reg: 0.00000 | Acc: 39.247% (12107/30848)\n",
      "270 1.5657109022140503\n",
      "270 391 Loss: 1.698 | Reg: 0.00000 | Acc: 39.054% (13547/34688)\n",
      "300 1.5573976039886475\n",
      "300 391 Loss: 1.695 | Reg: 0.00000 | Acc: 39.127% (15075/38528)\n",
      "330 1.608772873878479\n",
      "330 391 Loss: 1.685 | Reg: 0.00000 | Acc: 39.594% (16775/42368)\n",
      "360 1.5544190406799316\n",
      "360 391 Loss: 1.693 | Reg: 0.00000 | Acc: 39.335% (18176/46208)\n",
      "390 2.5944457054138184\n",
      "390 391 Loss: 1.687 | Reg: 0.00000 | Acc: 39.644% (19822/50000)\n",
      "0 500 Loss: 1.341 | Acc: 58.000% (58/100)\n",
      "10 500 Loss: 1.346 | Acc: 54.091% (595/1100)\n",
      "20 500 Loss: 1.360 | Acc: 53.190% (1117/2100)\n",
      "30 500 Loss: 1.358 | Acc: 53.097% (1646/3100)\n",
      "40 500 Loss: 1.359 | Acc: 52.561% (2155/4100)\n",
      "50 500 Loss: 1.364 | Acc: 52.510% (2678/5100)\n",
      "60 500 Loss: 1.361 | Acc: 52.361% (3194/6100)\n",
      "70 500 Loss: 1.363 | Acc: 52.056% (3696/7100)\n",
      "80 500 Loss: 1.364 | Acc: 52.148% (4224/8100)\n",
      "90 500 Loss: 1.363 | Acc: 52.242% (4754/9100)\n",
      "100 500 Loss: 1.361 | Acc: 52.356% (5288/10100)\n",
      "110 500 Loss: 1.366 | Acc: 52.270% (5802/11100)\n",
      "120 500 Loss: 1.364 | Acc: 52.397% (6340/12100)\n",
      "130 500 Loss: 1.366 | Acc: 52.290% (6850/13100)\n",
      "140 500 Loss: 1.370 | Acc: 52.142% (7352/14100)\n",
      "150 500 Loss: 1.374 | Acc: 51.967% (7847/15100)\n",
      "160 500 Loss: 1.379 | Acc: 51.814% (8342/16100)\n",
      "170 500 Loss: 1.376 | Acc: 51.994% (8891/17100)\n",
      "180 500 Loss: 1.375 | Acc: 52.066% (9424/18100)\n",
      "190 500 Loss: 1.374 | Acc: 52.031% (9938/19100)\n",
      "200 500 Loss: 1.376 | Acc: 51.841% (10420/20100)\n",
      "210 500 Loss: 1.376 | Acc: 51.806% (10931/21100)\n",
      "220 500 Loss: 1.376 | Acc: 51.814% (11451/22100)\n",
      "230 500 Loss: 1.375 | Acc: 51.874% (11983/23100)\n",
      "240 500 Loss: 1.374 | Acc: 51.934% (12516/24100)\n",
      "250 500 Loss: 1.373 | Acc: 51.948% (13039/25100)\n",
      "260 500 Loss: 1.373 | Acc: 51.950% (13559/26100)\n",
      "270 500 Loss: 1.374 | Acc: 51.934% (14074/27100)\n",
      "280 500 Loss: 1.375 | Acc: 51.861% (14573/28100)\n",
      "290 500 Loss: 1.374 | Acc: 51.852% (15089/29100)\n",
      "300 500 Loss: 1.373 | Acc: 51.877% (15615/30100)\n",
      "310 500 Loss: 1.373 | Acc: 51.820% (16116/31100)\n",
      "320 500 Loss: 1.373 | Acc: 51.810% (16631/32100)\n",
      "330 500 Loss: 1.376 | Acc: 51.710% (17116/33100)\n",
      "340 500 Loss: 1.378 | Acc: 51.636% (17608/34100)\n",
      "350 500 Loss: 1.377 | Acc: 51.689% (18143/35100)\n",
      "360 500 Loss: 1.377 | Acc: 51.715% (18669/36100)\n",
      "370 500 Loss: 1.378 | Acc: 51.674% (19171/37100)\n",
      "380 500 Loss: 1.378 | Acc: 51.635% (19673/38100)\n",
      "390 500 Loss: 1.378 | Acc: 51.657% (20198/39100)\n",
      "400 500 Loss: 1.378 | Acc: 51.653% (20713/40100)\n",
      "410 500 Loss: 1.379 | Acc: 51.652% (21229/41100)\n",
      "420 500 Loss: 1.379 | Acc: 51.660% (21749/42100)\n",
      "430 500 Loss: 1.378 | Acc: 51.645% (22259/43100)\n",
      "440 500 Loss: 1.378 | Acc: 51.615% (22762/44100)\n",
      "450 500 Loss: 1.379 | Acc: 51.605% (23274/45100)\n",
      "460 500 Loss: 1.378 | Acc: 51.644% (23808/46100)\n",
      "470 500 Loss: 1.379 | Acc: 51.626% (24316/47100)\n",
      "480 500 Loss: 1.379 | Acc: 51.593% (24816/48100)\n",
      "490 500 Loss: 1.379 | Acc: 51.560% (25316/49100)\n",
      "\n",
      "Epoch: 6\n",
      "0 1.2444508075714111\n",
      "0 391 Loss: 1.244 | Reg: 0.00000 | Acc: 60.156% (77/128)\n",
      "30 1.3226425647735596\n",
      "30 391 Loss: 1.664 | Reg: 0.00000 | Acc: 41.028% (1628/3968)\n",
      "60 2.1032209396362305\n",
      "60 391 Loss: 1.582 | Reg: 0.00000 | Acc: 43.532% (3399/7808)\n",
      "90 1.3990920782089233\n",
      "90 391 Loss: 1.551 | Reg: 0.00000 | Acc: 45.115% (5255/11648)\n",
      "120 1.9349063634872437\n",
      "120 391 Loss: 1.563 | Reg: 0.00000 | Acc: 44.551% (6900/15488)\n",
      "150 1.5025913715362549\n",
      "150 391 Loss: 1.571 | Reg: 0.00000 | Acc: 44.102% (8524/19328)\n",
      "180 1.7624999284744263\n",
      "180 391 Loss: 1.593 | Reg: 0.00000 | Acc: 43.357% (10045/23168)\n",
      "210 2.161790609359741\n",
      "210 391 Loss: 1.593 | Reg: 0.00000 | Acc: 43.032% (11622/27008)\n",
      "240 1.2078018188476562\n",
      "240 391 Loss: 1.587 | Reg: 0.00000 | Acc: 43.322% (13364/30848)\n",
      "270 1.964359998703003\n",
      "270 391 Loss: 1.597 | Reg: 0.00000 | Acc: 43.179% (14978/34688)\n",
      "300 1.363598346710205\n",
      "300 391 Loss: 1.595 | Reg: 0.00000 | Acc: 43.265% (16669/38528)\n",
      "330 1.9090745449066162\n",
      "330 391 Loss: 1.602 | Reg: 0.00000 | Acc: 43.099% (18260/42368)\n",
      "360 2.120720863342285\n",
      "360 391 Loss: 1.607 | Reg: 0.00000 | Acc: 43.133% (19931/46208)\n",
      "390 1.3102078437805176\n",
      "390 391 Loss: 1.605 | Reg: 0.00000 | Acc: 43.190% (21595/50000)\n",
      "0 500 Loss: 1.456 | Acc: 48.000% (48/100)\n",
      "10 500 Loss: 1.392 | Acc: 53.909% (593/1100)\n",
      "20 500 Loss: 1.394 | Acc: 53.762% (1129/2100)\n",
      "30 500 Loss: 1.388 | Acc: 53.065% (1645/3100)\n",
      "40 500 Loss: 1.392 | Acc: 52.829% (2166/4100)\n",
      "50 500 Loss: 1.397 | Acc: 52.529% (2679/5100)\n",
      "60 500 Loss: 1.397 | Acc: 52.672% (3213/6100)\n",
      "70 500 Loss: 1.397 | Acc: 52.775% (3747/7100)\n",
      "80 500 Loss: 1.397 | Acc: 52.568% (4258/8100)\n",
      "90 500 Loss: 1.397 | Acc: 52.582% (4785/9100)\n",
      "100 500 Loss: 1.396 | Acc: 52.653% (5318/10100)\n",
      "110 500 Loss: 1.396 | Acc: 52.775% (5858/11100)\n",
      "120 500 Loss: 1.396 | Acc: 52.769% (6385/12100)\n",
      "130 500 Loss: 1.398 | Acc: 52.550% (6884/13100)\n",
      "140 500 Loss: 1.401 | Acc: 52.567% (7412/14100)\n",
      "150 500 Loss: 1.404 | Acc: 52.444% (7919/15100)\n",
      "160 500 Loss: 1.407 | Acc: 52.335% (8426/16100)\n",
      "170 500 Loss: 1.407 | Acc: 52.439% (8967/17100)\n",
      "180 500 Loss: 1.406 | Acc: 52.459% (9495/18100)\n",
      "190 500 Loss: 1.406 | Acc: 52.576% (10042/19100)\n",
      "200 500 Loss: 1.406 | Acc: 52.507% (10554/20100)\n",
      "210 500 Loss: 1.408 | Acc: 52.389% (11054/21100)\n",
      "220 500 Loss: 1.406 | Acc: 52.403% (11581/22100)\n",
      "230 500 Loss: 1.406 | Acc: 52.346% (12092/23100)\n",
      "240 500 Loss: 1.406 | Acc: 52.419% (12633/24100)\n",
      "250 500 Loss: 1.404 | Acc: 52.502% (13178/25100)\n",
      "260 500 Loss: 1.404 | Acc: 52.421% (13682/26100)\n",
      "270 500 Loss: 1.405 | Acc: 52.406% (14202/27100)\n",
      "280 500 Loss: 1.405 | Acc: 52.377% (14718/28100)\n",
      "290 500 Loss: 1.404 | Acc: 52.364% (15238/29100)\n",
      "300 500 Loss: 1.404 | Acc: 52.419% (15778/30100)\n",
      "310 500 Loss: 1.403 | Acc: 52.441% (16309/31100)\n",
      "320 500 Loss: 1.404 | Acc: 52.452% (16837/32100)\n",
      "330 500 Loss: 1.405 | Acc: 52.411% (17348/33100)\n",
      "340 500 Loss: 1.407 | Acc: 52.290% (17831/34100)\n",
      "350 500 Loss: 1.407 | Acc: 52.285% (18352/35100)\n",
      "360 500 Loss: 1.407 | Acc: 52.296% (18879/36100)\n",
      "370 500 Loss: 1.407 | Acc: 52.329% (19414/37100)\n",
      "380 500 Loss: 1.407 | Acc: 52.278% (19918/38100)\n",
      "390 500 Loss: 1.406 | Acc: 52.279% (20441/39100)\n",
      "400 500 Loss: 1.407 | Acc: 52.269% (20960/40100)\n",
      "410 500 Loss: 1.408 | Acc: 52.221% (21463/41100)\n",
      "420 500 Loss: 1.408 | Acc: 52.204% (21978/42100)\n",
      "430 500 Loss: 1.408 | Acc: 52.200% (22498/43100)\n",
      "440 500 Loss: 1.408 | Acc: 52.150% (22998/44100)\n",
      "450 500 Loss: 1.408 | Acc: 52.177% (23532/45100)\n",
      "460 500 Loss: 1.408 | Acc: 52.234% (24080/46100)\n",
      "470 500 Loss: 1.408 | Acc: 52.202% (24587/47100)\n",
      "480 500 Loss: 1.408 | Acc: 52.173% (25095/48100)\n",
      "490 500 Loss: 1.407 | Acc: 52.214% (25637/49100)\n",
      "\n",
      "Epoch: 7\n",
      "0 1.4295144081115723\n",
      "0 391 Loss: 1.430 | Reg: 0.00000 | Acc: 45.312% (58/128)\n",
      "30 1.928938388824463\n",
      "30 391 Loss: 1.479 | Reg: 0.00000 | Acc: 47.177% (1872/3968)\n",
      "60 1.2613859176635742\n",
      "60 391 Loss: 1.486 | Reg: 0.00000 | Acc: 47.106% (3678/7808)\n",
      "90 1.3913530111312866\n",
      "90 391 Loss: 1.505 | Reg: 0.00000 | Acc: 46.231% (5385/11648)\n",
      "120 2.1620123386383057\n",
      "120 391 Loss: 1.517 | Reg: 0.00000 | Acc: 45.500% (7047/15488)\n",
      "150 1.0755329132080078\n",
      "150 391 Loss: 1.504 | Reg: 0.00000 | Acc: 45.840% (8860/19328)\n",
      "180 1.8324387073516846\n",
      "180 391 Loss: 1.515 | Reg: 0.00000 | Acc: 45.822% (10616/23168)\n",
      "210 1.1769922971725464\n",
      "210 391 Loss: 1.532 | Reg: 0.00000 | Acc: 45.509% (12291/27008)\n",
      "240 1.1883842945098877\n",
      "240 391 Loss: 1.537 | Reg: 0.00000 | Acc: 45.692% (14095/30848)\n",
      "270 1.3305046558380127\n",
      "270 391 Loss: 1.546 | Reg: 0.00000 | Acc: 45.480% (15776/34688)\n",
      "300 1.2891803979873657\n",
      "300 391 Loss: 1.546 | Reg: 0.00000 | Acc: 45.606% (17571/38528)\n",
      "330 2.1636247634887695\n",
      "330 391 Loss: 1.548 | Reg: 0.00000 | Acc: 45.648% (19340/42368)\n",
      "360 2.2971253395080566\n",
      "360 391 Loss: 1.546 | Reg: 0.00000 | Acc: 45.877% (21199/46208)\n",
      "390 2.118011474609375\n",
      "390 391 Loss: 1.537 | Reg: 0.00000 | Acc: 46.306% (23153/50000)\n",
      "0 500 Loss: 1.218 | Acc: 56.000% (56/100)\n",
      "10 500 Loss: 1.210 | Acc: 57.455% (632/1100)\n",
      "20 500 Loss: 1.213 | Acc: 59.619% (1252/2100)\n",
      "30 500 Loss: 1.207 | Acc: 59.452% (1843/3100)\n",
      "40 500 Loss: 1.213 | Acc: 58.805% (2411/4100)\n",
      "50 500 Loss: 1.212 | Acc: 58.941% (3006/5100)\n",
      "60 500 Loss: 1.216 | Acc: 58.689% (3580/6100)\n",
      "70 500 Loss: 1.216 | Acc: 58.606% (4161/7100)\n",
      "80 500 Loss: 1.215 | Acc: 58.765% (4760/8100)\n",
      "90 500 Loss: 1.216 | Acc: 58.758% (5347/9100)\n",
      "100 500 Loss: 1.214 | Acc: 58.703% (5929/10100)\n",
      "110 500 Loss: 1.216 | Acc: 58.775% (6524/11100)\n",
      "120 500 Loss: 1.215 | Acc: 58.702% (7103/12100)\n",
      "130 500 Loss: 1.217 | Acc: 58.649% (7683/13100)\n",
      "140 500 Loss: 1.219 | Acc: 58.695% (8276/14100)\n",
      "150 500 Loss: 1.222 | Acc: 58.483% (8831/15100)\n",
      "160 500 Loss: 1.225 | Acc: 58.335% (9392/16100)\n",
      "170 500 Loss: 1.224 | Acc: 58.392% (9985/17100)\n",
      "180 500 Loss: 1.223 | Acc: 58.453% (10580/18100)\n",
      "190 500 Loss: 1.222 | Acc: 58.660% (11204/19100)\n",
      "200 500 Loss: 1.223 | Acc: 58.607% (11780/20100)\n",
      "210 500 Loss: 1.223 | Acc: 58.531% (12350/21100)\n",
      "220 500 Loss: 1.222 | Acc: 58.638% (12959/22100)\n",
      "230 500 Loss: 1.223 | Acc: 58.545% (13524/23100)\n",
      "240 500 Loss: 1.220 | Acc: 58.664% (14138/24100)\n",
      "250 500 Loss: 1.218 | Acc: 58.717% (14738/25100)\n",
      "260 500 Loss: 1.218 | Acc: 58.739% (15331/26100)\n",
      "270 500 Loss: 1.218 | Acc: 58.812% (15938/27100)\n",
      "280 500 Loss: 1.218 | Acc: 58.722% (16501/28100)\n",
      "290 500 Loss: 1.217 | Acc: 58.763% (17100/29100)\n",
      "300 500 Loss: 1.216 | Acc: 58.797% (17698/30100)\n",
      "310 500 Loss: 1.216 | Acc: 58.817% (18292/31100)\n",
      "320 500 Loss: 1.217 | Acc: 58.760% (18862/32100)\n",
      "330 500 Loss: 1.219 | Acc: 58.650% (19413/33100)\n",
      "340 500 Loss: 1.221 | Acc: 58.540% (19962/34100)\n",
      "350 500 Loss: 1.221 | Acc: 58.573% (20559/35100)\n",
      "360 500 Loss: 1.221 | Acc: 58.612% (21159/36100)\n",
      "370 500 Loss: 1.222 | Acc: 58.593% (21738/37100)\n",
      "380 500 Loss: 1.223 | Acc: 58.530% (22300/38100)\n",
      "390 500 Loss: 1.222 | Acc: 58.529% (22885/39100)\n",
      "400 500 Loss: 1.223 | Acc: 58.526% (23469/40100)\n",
      "410 500 Loss: 1.224 | Acc: 58.487% (24038/41100)\n",
      "420 500 Loss: 1.224 | Acc: 58.508% (24632/42100)\n",
      "430 500 Loss: 1.223 | Acc: 58.510% (25218/43100)\n",
      "440 500 Loss: 1.223 | Acc: 58.474% (25787/44100)\n",
      "450 500 Loss: 1.224 | Acc: 58.452% (26362/45100)\n",
      "460 500 Loss: 1.223 | Acc: 58.499% (26968/46100)\n",
      "470 500 Loss: 1.223 | Acc: 58.514% (27560/47100)\n",
      "480 500 Loss: 1.224 | Acc: 58.478% (28128/48100)\n",
      "490 500 Loss: 1.223 | Acc: 58.562% (28754/49100)\n",
      "\n",
      "Epoch: 8\n",
      "0 1.4740729331970215\n",
      "0 391 Loss: 1.474 | Reg: 0.00000 | Acc: 50.000% (64/128)\n",
      "30 1.3577003479003906\n",
      "30 391 Loss: 1.482 | Reg: 0.00000 | Acc: 49.521% (1965/3968)\n",
      "60 1.0685652494430542\n",
      "60 391 Loss: 1.510 | Reg: 0.00000 | Acc: 47.682% (3723/7808)\n",
      "90 1.489551067352295\n",
      "90 391 Loss: 1.490 | Reg: 0.00000 | Acc: 48.128% (5606/11648)\n",
      "120 1.5280613899230957\n",
      "120 391 Loss: 1.488 | Reg: 0.00000 | Acc: 47.998% (7434/15488)\n",
      "150 1.2081676721572876\n",
      "150 391 Loss: 1.497 | Reg: 0.00000 | Acc: 47.465% (9174/19328)\n",
      "180 1.4450019598007202\n",
      "180 391 Loss: 1.473 | Reg: 0.00000 | Acc: 48.282% (11186/23168)\n",
      "210 1.9685720205307007\n",
      "210 391 Loss: 1.461 | Reg: 0.00000 | Acc: 48.637% (13136/27008)\n",
      "240 1.155348777770996\n",
      "240 391 Loss: 1.467 | Reg: 0.00000 | Acc: 48.476% (14954/30848)\n",
      "270 1.4940016269683838\n",
      "270 391 Loss: 1.469 | Reg: 0.00000 | Acc: 48.538% (16837/34688)\n",
      "300 1.1711554527282715\n",
      "300 391 Loss: 1.471 | Reg: 0.00000 | Acc: 48.645% (18742/38528)\n",
      "330 1.4600529670715332\n",
      "330 391 Loss: 1.483 | Reg: 0.00000 | Acc: 48.447% (20526/42368)\n",
      "360 2.3216984272003174\n",
      "360 391 Loss: 1.485 | Reg: 0.00000 | Acc: 48.405% (22367/46208)\n",
      "390 1.565482258796692\n",
      "390 391 Loss: 1.494 | Reg: 0.00000 | Acc: 48.044% (24022/50000)\n",
      "0 500 Loss: 1.224 | Acc: 64.000% (64/100)\n",
      "10 500 Loss: 1.204 | Acc: 62.455% (687/1100)\n",
      "20 500 Loss: 1.206 | Acc: 63.714% (1338/2100)\n",
      "30 500 Loss: 1.189 | Acc: 64.323% (1994/3100)\n",
      "40 500 Loss: 1.198 | Acc: 63.634% (2609/4100)\n",
      "50 500 Loss: 1.204 | Acc: 63.157% (3221/5100)\n",
      "60 500 Loss: 1.202 | Acc: 63.066% (3847/6100)\n",
      "70 500 Loss: 1.208 | Acc: 62.620% (4446/7100)\n",
      "80 500 Loss: 1.208 | Acc: 62.531% (5065/8100)\n",
      "90 500 Loss: 1.210 | Acc: 62.352% (5674/9100)\n",
      "100 500 Loss: 1.206 | Acc: 62.426% (6305/10100)\n",
      "110 500 Loss: 1.208 | Acc: 62.333% (6919/11100)\n",
      "120 500 Loss: 1.209 | Acc: 62.355% (7545/12100)\n",
      "130 500 Loss: 1.210 | Acc: 62.290% (8160/13100)\n",
      "140 500 Loss: 1.214 | Acc: 62.291% (8783/14100)\n",
      "150 500 Loss: 1.218 | Acc: 62.079% (9374/15100)\n",
      "160 500 Loss: 1.219 | Acc: 61.994% (9981/16100)\n",
      "170 500 Loss: 1.218 | Acc: 62.058% (10612/17100)\n",
      "180 500 Loss: 1.219 | Acc: 62.011% (11224/18100)\n",
      "190 500 Loss: 1.218 | Acc: 62.068% (11855/19100)\n",
      "200 500 Loss: 1.220 | Acc: 61.970% (12456/20100)\n",
      "210 500 Loss: 1.221 | Acc: 61.910% (13063/21100)\n",
      "220 500 Loss: 1.219 | Acc: 61.977% (13697/22100)\n",
      "230 500 Loss: 1.219 | Acc: 61.987% (14319/23100)\n",
      "240 500 Loss: 1.218 | Acc: 61.992% (14940/24100)\n",
      "250 500 Loss: 1.216 | Acc: 62.064% (15578/25100)\n",
      "260 500 Loss: 1.216 | Acc: 62.038% (16192/26100)\n",
      "270 500 Loss: 1.216 | Acc: 62.092% (16827/27100)\n",
      "280 500 Loss: 1.216 | Acc: 62.085% (17446/28100)\n",
      "290 500 Loss: 1.215 | Acc: 62.093% (18069/29100)\n",
      "300 500 Loss: 1.214 | Acc: 62.156% (18709/30100)\n",
      "310 500 Loss: 1.215 | Acc: 62.096% (19312/31100)\n",
      "320 500 Loss: 1.216 | Acc: 62.059% (19921/32100)\n",
      "330 500 Loss: 1.218 | Acc: 61.994% (20520/33100)\n",
      "340 500 Loss: 1.220 | Acc: 61.900% (21108/34100)\n",
      "350 500 Loss: 1.219 | Acc: 61.934% (21739/35100)\n",
      "360 500 Loss: 1.219 | Acc: 61.950% (22364/36100)\n",
      "370 500 Loss: 1.220 | Acc: 61.954% (22985/37100)\n",
      "380 500 Loss: 1.221 | Acc: 61.874% (23574/38100)\n",
      "390 500 Loss: 1.221 | Acc: 61.893% (24200/39100)\n",
      "400 500 Loss: 1.220 | Acc: 61.920% (24830/40100)\n",
      "410 500 Loss: 1.221 | Acc: 61.886% (25435/41100)\n",
      "420 500 Loss: 1.220 | Acc: 61.888% (26055/42100)\n",
      "430 500 Loss: 1.220 | Acc: 61.912% (26684/43100)\n",
      "440 500 Loss: 1.220 | Acc: 61.893% (27295/44100)\n",
      "450 500 Loss: 1.220 | Acc: 61.878% (27907/45100)\n",
      "460 500 Loss: 1.219 | Acc: 61.939% (28554/46100)\n",
      "470 500 Loss: 1.219 | Acc: 61.934% (29171/47100)\n",
      "480 500 Loss: 1.219 | Acc: 61.944% (29795/48100)\n",
      "490 500 Loss: 1.218 | Acc: 62.022% (30453/49100)\n",
      "\n",
      "Epoch: 9\n",
      "0 1.8100636005401611\n",
      "0 391 Loss: 1.810 | Reg: 0.00000 | Acc: 31.250% (40/128)\n",
      "30 1.0881311893463135\n",
      "30 391 Loss: 1.616 | Reg: 0.00000 | Acc: 42.112% (1671/3968)\n",
      "60 1.2026135921478271\n",
      "60 391 Loss: 1.497 | Reg: 0.00000 | Acc: 46.324% (3617/7808)\n",
      "90 1.9083961248397827\n",
      "90 391 Loss: 1.458 | Reg: 0.00000 | Acc: 47.630% (5548/11648)\n",
      "120 2.160477638244629\n",
      "120 391 Loss: 1.497 | Reg: 0.00000 | Acc: 46.985% (7277/15488)\n",
      "150 1.2115967273712158\n",
      "150 391 Loss: 1.481 | Reg: 0.00000 | Acc: 47.863% (9251/19328)\n",
      "180 1.2150553464889526\n",
      "180 391 Loss: 1.513 | Reg: 0.00000 | Acc: 46.879% (10861/23168)\n",
      "210 1.478805422782898\n",
      "210 391 Loss: 1.528 | Reg: 0.00000 | Acc: 46.805% (12641/27008)\n",
      "240 1.0661532878875732\n",
      "240 391 Loss: 1.522 | Reg: 0.00000 | Acc: 47.274% (14583/30848)\n",
      "270 2.059490203857422\n",
      "270 391 Loss: 1.507 | Reg: 0.00000 | Acc: 47.890% (16612/34688)\n",
      "300 0.947658121585846\n",
      "300 391 Loss: 1.498 | Reg: 0.00000 | Acc: 48.248% (18589/38528)\n",
      "330 1.9750653505325317\n",
      "330 391 Loss: 1.490 | Reg: 0.00000 | Acc: 48.525% (20559/42368)\n",
      "360 1.0188347101211548\n",
      "360 391 Loss: 1.488 | Reg: 0.00000 | Acc: 48.552% (22435/46208)\n",
      "390 1.126692295074463\n",
      "390 391 Loss: 1.479 | Reg: 0.00000 | Acc: 48.730% (24365/50000)\n",
      "0 500 Loss: 1.289 | Acc: 58.000% (58/100)\n",
      "10 500 Loss: 1.176 | Acc: 61.909% (681/1100)\n",
      "20 500 Loss: 1.175 | Acc: 63.238% (1328/2100)\n",
      "30 500 Loss: 1.166 | Acc: 63.258% (1961/3100)\n",
      "40 500 Loss: 1.174 | Acc: 62.659% (2569/4100)\n",
      "50 500 Loss: 1.179 | Acc: 62.647% (3195/5100)\n",
      "60 500 Loss: 1.177 | Acc: 62.475% (3811/6100)\n",
      "70 500 Loss: 1.181 | Acc: 62.225% (4418/7100)\n",
      "80 500 Loss: 1.184 | Acc: 61.938% (5017/8100)\n",
      "90 500 Loss: 1.186 | Acc: 61.967% (5639/9100)\n",
      "100 500 Loss: 1.182 | Acc: 62.040% (6266/10100)\n",
      "110 500 Loss: 1.181 | Acc: 62.054% (6888/11100)\n",
      "120 500 Loss: 1.181 | Acc: 62.074% (7511/12100)\n",
      "130 500 Loss: 1.182 | Acc: 62.000% (8122/13100)\n",
      "140 500 Loss: 1.187 | Acc: 62.050% (8749/14100)\n",
      "150 500 Loss: 1.190 | Acc: 61.954% (9355/15100)\n",
      "160 500 Loss: 1.194 | Acc: 61.826% (9954/16100)\n",
      "170 500 Loss: 1.193 | Acc: 61.865% (10579/17100)\n",
      "180 500 Loss: 1.190 | Acc: 61.917% (11207/18100)\n",
      "190 500 Loss: 1.189 | Acc: 61.921% (11827/19100)\n",
      "200 500 Loss: 1.192 | Acc: 61.791% (12420/20100)\n",
      "210 500 Loss: 1.193 | Acc: 61.806% (13041/21100)\n",
      "220 500 Loss: 1.192 | Acc: 61.910% (13682/22100)\n",
      "230 500 Loss: 1.192 | Acc: 61.831% (14283/23100)\n",
      "240 500 Loss: 1.191 | Acc: 61.846% (14905/24100)\n",
      "250 500 Loss: 1.190 | Acc: 61.876% (15531/25100)\n",
      "260 500 Loss: 1.190 | Acc: 61.843% (16141/26100)\n",
      "270 500 Loss: 1.189 | Acc: 61.904% (16776/27100)\n",
      "280 500 Loss: 1.189 | Acc: 61.836% (17376/28100)\n",
      "290 500 Loss: 1.188 | Acc: 61.869% (18004/29100)\n",
      "300 500 Loss: 1.187 | Acc: 61.934% (18642/30100)\n",
      "310 500 Loss: 1.186 | Acc: 61.920% (19257/31100)\n",
      "320 500 Loss: 1.187 | Acc: 61.860% (19857/32100)\n",
      "330 500 Loss: 1.188 | Acc: 61.801% (20456/33100)\n",
      "340 500 Loss: 1.189 | Acc: 61.721% (21047/34100)\n",
      "350 500 Loss: 1.189 | Acc: 61.769% (21681/35100)\n",
      "360 500 Loss: 1.189 | Acc: 61.770% (22299/36100)\n",
      "370 500 Loss: 1.189 | Acc: 61.819% (22935/37100)\n",
      "380 500 Loss: 1.190 | Acc: 61.717% (23514/38100)\n",
      "390 500 Loss: 1.189 | Acc: 61.760% (24148/39100)\n",
      "400 500 Loss: 1.188 | Acc: 61.793% (24779/40100)\n",
      "410 500 Loss: 1.189 | Acc: 61.762% (25384/41100)\n",
      "420 500 Loss: 1.188 | Acc: 61.793% (26015/42100)\n",
      "430 500 Loss: 1.188 | Acc: 61.817% (26643/43100)\n",
      "440 500 Loss: 1.188 | Acc: 61.760% (27236/44100)\n",
      "450 500 Loss: 1.189 | Acc: 61.745% (27847/45100)\n",
      "460 500 Loss: 1.188 | Acc: 61.768% (28475/46100)\n",
      "470 500 Loss: 1.188 | Acc: 61.771% (29094/47100)\n",
      "480 500 Loss: 1.188 | Acc: 61.792% (29722/48100)\n",
      "490 500 Loss: 1.187 | Acc: 61.819% (30353/49100)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss, reg_loss, train_acc = train(epoch)\n",
    "    test_loss, test_acc = test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "24dab280bd70051585d7622ca66d7bb6e1ae35da9861b9af5a523706ae2b44be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
